{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phoenix AI & Machine Learning Group</h1>\n",
    "<h2>Let's get Neural: Solving the MNIST Dataset (Python) Part 1/3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Community Support</h4>\n",
    "\n",
    "As always, a generous shoutout to the community, without these contributions, my understanding of Neural Networks would have taken much longer! <br>\n",
    "\n",
    "Most code directly copied from @Trask: http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "<i>Other Works Cited: </i>\n",
    "- http://cs231n.github.io/neural-networks-1/\n",
    "- http://www.cse.unsw.edu.au/~billw/mldict.html\n",
    "- https://en.wikipedia.org/wiki/Backpropagation\n",
    "- https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- http://squall0032.tumblr.com/post/77300791096/plotting-a-sigmoid-function-using\n",
    "- http://stevenmiller888.github.io/mind-how-to-build-a-neural-network/\n",
    "- http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3 Layer - XOR Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Inputs\t|Output|\n",
    "|-------|------|\n",
    "|0\t0\t1\t|1|\n",
    "|0\t1\t1\t|0|\n",
    "|1\t0\t1\t|0|\n",
    "|1\t1\t1\t|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layers: \n",
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "\n",
      "Expected Output: \n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output dataset\n",
    "y = np.array([[1,\n",
    "               0,\n",
    "               0,\n",
    "               1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print (\"Input Layers: \")\n",
    "print X\n",
    "print\n",
    "print (\"Expected Output: \")\n",
    "print y\n",
    "print\n",
    "print \"Output After Training:\"\n",
    "print l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So what happened?\n",
    "# Our Neural Network is simply unable to predict what the values should be!\n",
    "# Remember our \"Error Weighted Derivative\" ?\n",
    "# It isn't robust enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"pic1\" style=\"float:left;width:50%\"><img class=\"img-responsive\" width=\"100%\" src=\"https://iamtrask.github.io/img/rcnn.png\" alt=\"\"><br></div><div id=\"pic2\" style=\"float:right;width:50%;\"><img class=\"img-responsive\" width=\"100%\" src=\"https://iamtrask.github.io/img/margritti-this-is-not-a-pipe.jpg\" alt=\"\"><br></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Believe it or not, image recognition is a similar problem. \n",
    "# If one had 100 identically sized images of pipes and bicycles,\n",
    "# no individual pixel position would directly correlate with the presence of a bicycle or pipe. \n",
    "# The pixels might as well be random from a purely statistical point of view. \n",
    "# However, certain combinations of pixels are not random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.503589968097\n",
      "Error:0.010347374888\n",
      "Error:0.00680775185908\n",
      "Error:0.00535111040195\n",
      "Error:0.00452404113138\n",
      "Error:0.00397933174579\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "             [1,0,1],\n",
    "             [1,1,1]])\n",
    "                \n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for j in xrange(60000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print \"Error:\" + str(np.mean(np.abs(l2_error)))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99733349]\n",
      " [ 0.00409725]\n",
      " [ 0.00390993]\n",
      " [ 0.99632136]]\n"
     ]
    }
   ],
   "source": [
    "print l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Succes! Simply by adding additional layers to out network, \n",
    "# we were able to successfully train our network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
